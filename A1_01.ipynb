{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1 01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ato7pCn1UVLM"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import numpy as np # linear algebra\n",
        "import os # accessing directory structure\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpn6LEIWXovL"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jvxhsTyYkW5"
      },
      "source": [
        "print(os.listdir('/content/drive/MyDrive/audit_data'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP4KaCi0Y_k0"
      },
      "source": [
        "# Distribution graphs (histogram/bar graph) of column data\n",
        "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
        "    nunique = df.nunique()\n",
        "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
        "    nRow, nCol = df.shape\n",
        "    columnNames = list(df)\n",
        "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
        "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
        "    for i in range(min(nCol, nGraphShown)):\n",
        "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
        "        columnDf = df.iloc[:, i]\n",
        "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
        "            valueCounts = columnDf.value_counts()\n",
        "            valueCounts.plot.bar()\n",
        "        else:\n",
        "            columnDf.hist()\n",
        "        plt.ylabel('counts')\n",
        "        plt.xticks(rotation = 90)\n",
        "        plt.title(f'{columnNames[i]} (column {i})')\n",
        "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HvxGdPuTPDO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtlDkukGZPoK"
      },
      "source": [
        "# Correlation matrix\n",
        "def plotCorrelationMatrix(df, graphWidth):\n",
        "    filename = df.dataframeName\n",
        "    df = df.dropna('columns') # drop columns with NaN\n",
        "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
        "    if df.shape[1] < 2:\n",
        "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
        "        return\n",
        "    corr = df.corr()\n",
        "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
        "    corrMat = plt.matshow(corr, fignum = 1)\n",
        "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
        "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
        "    plt.gca().xaxis.tick_bottom()\n",
        "    plt.colorbar(corrMat)\n",
        "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GwMgZQRZVjp"
      },
      "source": [
        "# Scatter and density plots\n",
        "def plotScatterMatrix(df, plotSize, textSize):\n",
        "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
        "    # Remove rows and columns that would lead to df being singular\n",
        "    df = df.dropna('columns')\n",
        "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
        "    columnNames = list(df)\n",
        "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
        "        columnNames = columnNames[:10]\n",
        "    df = df[columnNames]\n",
        "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
        "    corrs = df.corr().values\n",
        "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
        "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
        "    plt.suptitle('Scatter and Density Plot')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC-ufnylZcP5"
      },
      "source": [
        "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/audit_data/audit_risk.csv', delimiter=',', nrows = nRowsRead)\n",
        "df1.dataframeName = 'audit_risk.csv'\n",
        "nRow, nCol = df1.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17hq11LRaJJl"
      },
      "source": [
        "df1.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSOYrfraTr_"
      },
      "source": [
        "plotPerColumnDistribution(df1, 10, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5ICVfaFaaeh"
      },
      "source": [
        "plotCorrelationMatrix(df1, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTDp0JtfaiIM"
      },
      "source": [
        "plotScatterMatrix(df1, 20, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C0cpAQ2aqil"
      },
      "source": [
        "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/audit_data/trial.csv', delimiter=',', nrows = nRowsRead)\n",
        "df2.dataframeName = 'trial.csv'\n",
        "nRow, nCol = df2.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0qKjhlbazmI"
      },
      "source": [
        "df2.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX79W6fOa79j"
      },
      "source": [
        "plotPerColumnDistribution(df2, 10, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gJQSZUKbD88"
      },
      "source": [
        "plotCorrelationMatrix(df2, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy30pqdLbM3w"
      },
      "source": [
        "plotScatterMatrix(df2, 20, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jK_wZypcFgD"
      },
      "source": [
        "# Importing libraries in Python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQCVBn3pcY0q"
      },
      "source": [
        "import seaborn as sns\n",
        "#get correlations of each features in dataset\n",
        "corrmat = df1.corr()\n",
        "top_corr_features = corrmat.index\n",
        "plt.figure(figsize=(20,20))\n",
        "#plot heat map\n",
        "g=sns.heatmap(df1[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWAzL_bBcufv"
      },
      "source": [
        "import seaborn as sns\n",
        "#get correlations of each features in dataset\n",
        "corrmat = df2.corr()\n",
        "top_corr_features = corrmat.index\n",
        "plt.figure(figsize=(20,20))\n",
        "#plot heat map\n",
        "g=sns.heatmap(df2[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLrAWyxqf6TK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('max_columns', None)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul_RQ9AHgEcz"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/audit_data/audit_risk.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_K3g7nAgNXQ"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE-QJObngSpl"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSyBkYKEgVX5"
      },
      "source": [
        "def preprocess_inputs(df):\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Fill missing value\n",
        "    df['Money_Value'] = df['Money_Value'].fillna(df['Money_Value'].mean())\n",
        "    \n",
        "    # One-hot encode the LOCATION_ID column\n",
        "    location_dummies = pd.get_dummies(df['LOCATION_ID'], prefix='location')\n",
        "    df = pd.concat([df, location_dummies], axis=1)\n",
        "    df = df.drop('LOCATION_ID', axis=1)\n",
        "    \n",
        "    # Split df into X and y\n",
        "    y = df['Risk']\n",
        "    X = df.drop('Risk', axis=1)\n",
        "    \n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n",
        "    \n",
        "    # Scale X\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjkdpBsKgf3M"
      },
      "source": [
        "X_train, X_test, y_train, y_test = preprocess_inputs(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otdt4Dnhgiex"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbRmovEIgsTT"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zvoYPFPgup6"
      },
      "source": [
        "models = {\n",
        "    \"                   Logistic Regression\": LogisticRegression(),\n",
        "    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    \"                         Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n",
        "    \"   Support Vector Machine (RBF Kernel)\": SVC(),\n",
        "    \"                        Neural Network\": MLPClassifier(),\n",
        "    \"                         Random Forest\": RandomForestClassifier(),\n",
        "    \"                     Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    print(name + \" trained.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H2sCRhpg5Kj"
      },
      "source": [
        "for name, model in models.items():\n",
        "    print(name + \": {:.2f}%\".format(model.score(X_test, y_test) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YqHbo6ShUVw"
      },
      "source": [
        "#Logistic Regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_model= logreg.predict(X_test)\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, y_model))\n",
        "print(classification_report(y_test, y_model))\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(y_test, y_model)\n",
        "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
        "plt.xlabel('predicted value')\n",
        "plt.ylabel('true value')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPjQzLL-htbt"
      },
      "source": [
        "#Decsion tree classification with decision boundary\n",
        "#Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#Importing the dataset\n",
        "y = df1['Risk']\n",
        "X = df1.iloc[:, [8, -1]].values\n",
        "#Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "#Training the decision tree model on the Training set\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "#Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "#Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "#Visualising the Training set results\n",
        "from matplotlib.colors import ListedColormap\n",
        "X = df1.iloc[ :, :-1].values\n",
        "X_set, y_set = X_train, y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Decision Tree Classification (Training set)')\n",
        "plt.xlabel('Total')\n",
        "plt.ylabel('Risk')\n",
        "plt.legend()\n",
        "plt.show() \n",
        "#Visualising the Test set results\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_test, y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Decision Tree Classification (Test set)')\n",
        "plt.xlabel('Total')\n",
        "plt.ylabel('Risk')\n",
        "plt.legend()\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGCo7qwAifqP"
      },
      "source": [
        "#Decsion tree classification with decision boundary\n",
        "#Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#Importing the dataset\n",
        "y = df2['Risk']\n",
        "X = df2.iloc[:, [8, -1]].values\n",
        "#Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "#Training the decision tree model on the Training set\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "#Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "#Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "#Visualising the Training set results\n",
        "from matplotlib.colors import ListedColormap\n",
        "X = df2.iloc[ :, :-1].values\n",
        "X_set, y_set = X_train, y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Decision Tree Classification (Training set)')\n",
        "plt.xlabel('Total')\n",
        "plt.ylabel('Risk')\n",
        "plt.legend()\n",
        "plt.show() \n",
        "#Visualising the Test set results\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_test, y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Decision Tree Classification (Test set)')\n",
        "plt.xlabel('Total')\n",
        "plt.ylabel('Risk')\n",
        "plt.legend()\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}